% Template for a Computer Science Tripos Part II project dissertation
\documentclass[12pt,a4paper,twoside,openright]{report}
\usepackage[backend=biber]{biblatex}
\usepackage[pdfborder={0 0 0}]{hyperref}    % turns references into hyperlinks
\usepackage[margin=25mm]{geometry}  % adjusts page layout
\usepackage{parskip}
\usepackage{graphicx}  % allows inclusion of PDF, PNG and JPG images
\usepackage{verbatim}
\usepackage{docmute}   % only needed to allow inclusion of proposal.tex
\usepackage{setspace}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{float}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{amsthm}
\usepackage{subcaption}
\usepackage{url}
\usepackage[justification=centering]{caption}
\usepackage[utf8]{inputenc}    % utf8 support       %!!!!!!!!!!!!!!!!!!!!
\usepackage[T1]{fontenc}
\usepackage{booktabs}
\usepackage{array}   % for \newcolumntype macro
\usepackage{makecell}
\usepackage{xcolor}

\addbibresource{references.bib}
\usetikzlibrary{arrows,backgrounds}
\usetikzlibrary{positioning}
\usetikzlibrary{decorations.pathreplacing}
\usetikzlibrary{arrows}
\usetikzlibrary{backgrounds}
\usetikzlibrary{matrix}
\tikzstyle{place}=[circle, draw=black, minimum size = 8mm]
\tikzset{>=latex}
\usepgflibrary{shapes.multipart}
\newcolumntype{C}{>{$}c<{$}}
\newcolumntype{R}{>{$}r<{$}}


\raggedbottom                           % try to avoid widows and orphans
\sloppy
\clubpenalty1000%
\widowpenalty1000%

\renewcommand{\baselinestretch}{1.1}    % adjust line spacing to make
                                        % more readable
\renewcommand{\vec}[1]{\bm{#1}}
\renewcommand{\thealgorithm}{\arabic{chapter}.\arabic{algorithm}}
\newtheorem*{defn}{Definition} 

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title

\pagestyle{empty}

\rightline{\LARGE \textbf{Charles London}}

\vspace*{60mm}
\begin{center}
\Huge
\textbf{A tool for prediction of phenotype from cell genotype} \\[5mm]
{\setstretch{0.5}\Large Alternate: Semi-supervised learning with autoencoders for classification of gene expression data} \\[5mm]
Computer Science Tripos -- Part II \\[5mm]
Trinity College \\[5mm]
\today  % today's date
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Proforma, table of contents and list of figures

\pagestyle{plain}

\chapter*{Proforma}

{\large
\begin{tabular}{ll}
Name:               & \bf Charles London                      \\
College:            & \bf Trinity College                     \\
Project Title:      & \bf A tool for phenotype prediction from cell genotype  \\
Examination:        & \bf Computer Science Tripos -- Part II, March 2019  \\
Word Count:         & \bf 1587\footnotemark[1]  \\
Project Originator: & Prof P.~Li\'o                   \\
Supervisors:         & Prof P.~Li\'o \& Helena Andres Terre                   \\ 
\end{tabular}
}
\footnotetext[1]{This word count was computed
by \texttt{detex diss.tex | tr -cd '0-9A-Za-z $\tt\backslash$n' | wc -w}
}
\stepcounter{footnote}

\section*{Original Aims of the Project}

\section*{Work Completed}

All that has been completed appears in this dissertation.

\section*{Special Difficulties}

Learning how to incorporate encapulated postscript into a \LaTeX\
document on both Ubuntu Linux and OS X.
 
\newpage
\section*{Declaration}

I, [Name] of [College], being a candidate for Part II of the Computer
Science Tripos [or the Diploma in Computer Science], hereby declare
that this dissertation and the work described in it are my own work,
unaided except as may be specified below, and that the dissertation
does not contain material that has already been used to any substantial
extent for a comparable purpose.

\bigskip
\leftline{Signed [signature]}

\medskip
\leftline{Date [date]}

\tableofcontents

\listoffigures

\newpage
\section*{Acknowledgements}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% now for the chapters

\pagestyle{headings}

\chapter{Introduction}

\textit{The stated aim of this project was to develop a semi-supervised autoencoder-based method for classifying 
cells into phenotypes \footnote{The observable characteristics and traits of an organism} using genetic data. To this end a range 
of autoencoder based semi-supervised models have been implemented, 
ranging from fairly simple to state-of-the-art. These models were then evaluated on selected gene expression datasets, including 
the Cancer Genome Atlas expression data.}

\section{Motivation}

Gene expression is the process of synthesising proteins from the gene via RNA. A transcriptome is the set of all RNA 
molecules in a cell or population of cells. While every cell with a nucleus in an organism has the same DNA and genes, the cells
differ in function, and this depends on which genes are being actively expressed. This in turn means the
transcriptome contains information from both genetic and epigenetic sources~\cite{Gibney2010}. Epigenetic differences are differences
in the phenotype without alterations to the DNA (e.g. DNA methylation). Therefore, as this project aims to classify cells into phenotypes, 
gene expression data is used.

Biological labs worldwide perform transcriptome analysis, resulting in huge amounts of gene expression data being
generated. Much of this is shared or available online, giving researchers access to huge amounts of data. The development
of RNA-Seq using next generation sequencing has also resulted in increased amounts of gene expression data, being more 
accurate and cost effective than previous methods. However,
while many of the datasets generated in different experiments may include transcriptomes for the same species of organism,
the majority of the time the experiments are measuring different phenotypes of the organism. This means that a 
researcher wanting to analyse or predict a specific phenotype is unable to use much of the available data, being limited
to only those labelled with the desired phenotype.

Semi-supervised learning attempts to leverage unlabelled data to improve the accuracy of the machine learning
algorithm on a supervised task. Autoencoders have a long history of being used in semi-supervised learning problems,
having been used early on to improve deep networks by pretraining them using stacked denoising autoencoders (Section~\ref{sdae})
and recently having been used to achieve state of the art semi-supervised performance as part of the ladder 
network (Section~\ref{ladder}). The main reason for their use is that autoencoders are good at learning 
important features of data in an unsupervised manner, and these features are often useful in improving 
supervised performance.

Therefore, with the use of semi-supervised autoencoder models it should be possible to leverage the data without
the desired phenotype to improve performance in predicting the phenotype.

The reason for choosing autoencoder-based models to use with gene expression data is that they are implemented
using neural networks. This is advantageous because neural networks work very well on non-linear data and are
also able to effectively analyse data with very high dimensionality (number of features). Transcriptomes can
often contain several thousand genes, and so any model used must be able to cope with this level of dimensionality.

\section{Related work}

Stacked denoising autoencoders have previously been used with gene expression data to derive the most informative
genes for distinguishing between healthy and cancerous cells~\cite{8217828}.

Likewise, variational autoencoders (Section~\ref{vae}) have been successfully used to extract a biologically relevant latent 
space from cancer transcriptomes ~\cite{Way2018ExtractingAB}. They and the semi-supervised variant (Section~\ref{ssVAE}) 
have also been used to model the change in the gene expression of tumours in response to certain drugs~\cite{10.1093/bioinformatics/btz158}.
he semi-supervised model in the paper, Dr.VAE, jointly models both the drug response and the treatment outcomes.

Ladder networks have also been used in biologically relevant ways, achieving state of the art accuracy in the binary cancer classification
problem using gene expression data~\cite{10.1007/978-3-319-78723-7_23}.

\chapter{Preparation}

\section{Neural networks}

This project involves using \textbf{autoencoders} as a basis for semi-supervised learning and classification of gene expression data. 
Autoencoder architectures are all based on \textbf{deep feedforward networks} so it is important to have a good understanding of the basic principles
of these networks.

\subsection{The perceptron}

The perceptron (also known as the \textbf{artificial neuron}~\cite{WhatareA61:online}) is the basic element of the neural network. It takes in inputs $(x_1, ..., x_n)$ and 
multiplies these by \textbf{weights} $(w_1, ..., w_n)$, giving a linear combination of the inputs $x_1w_1 + ... + x_nw_n$, before applying 
an \textbf{activation function} $\sigma$~\cite{Art_Int}.

It can be helpful to think of the inputs and weights as 
vectors $\vec{x}$ and $\vec{w}$, giving the combination as $\vec{x} \cdot \ \vec{w}$. A \textbf{bias} term $x_0=1$ may also be included and multiplied by 
weight $w_0$, giving $\vec{x} = [1, x_1, ..., x_n]$ and $\vec{w} = [w_0, w_1, ..., w_n]$. Intuitively the bias can be thought of as the intercept
on a graph - if the neuron should have an output when all the inputs are zero it will be unable to model this correctly without a bias.

The function computed by the neuron is then:
\begin{equation}
  y = \sigma(\vec{x} \cdot \vec{w})
\end{equation}

The activation function applied will depend on what the neuron is being used for.
The main activation functions used in this project are:
\begin{itemize}
  \item \textbf{Linear}
        \begin{equation}
          \sigma(z) = z
        \end{equation}
        Used in regression problems, when a real-valued output that can take any value is required.
  \item \textbf{ReLU}
        \begin{equation}
          \sigma(z) = max(0, z)
          \label{eq:relu}
        \end{equation}
        Used in the hidden layers of deep feedforward networks (described in the next section) to provide non-linearity, allowing the network to learn more 
        complicated functions. Has seen massive uptake due to performance benefits over sigmoid~\cite{relu}.
\end{itemize}

\subsection{Multilayer perceptrons}

Multilayer perceptrons are made up of multiple perceptrons arranged into \textbf{layers}. 
Each neuron in a layer has its own set of weights
$\vec{w}$ and takes the ouputs of the previous layer (or the inputs to the network if the neuron is in the first layer) $\vec{x}$ in order to compute the 
output value $y$ for the neuron. This value is then used as an input to the next layer, or part of the output of the network if the neuron is in
the output layer.
\begin{figure}[H]
  \begin{center}
      \begin{tikzpicture}
        \tikzstyle{place}=[circle, draw=black, minimum size = 4mm]
        
        % Input
        \draw node [dashed] at (0, 0) [place] (first_0) {$1$};
        \foreach \x in {1,...,3}
          \draw node at (0, -\x*1.2 - 0.6) [place] (first_\x) {$x_\x$};
        \foreach \x in {1,...,3}
          \draw [->] (-1.5, -\x*1.2 - 0.6) to (first_\x); 
        
        % Hidden 1
        \draw node [dashed] at (4, 0) [place] (second_0) {$1$};
        \foreach \x in {1,...,4}
          \node at (4, -\x*1.2) [place] (second_\x) {$h^{1}_\x$};

        % Hidden 2
        \draw node [dashed] at (8, 0) [place] (third_0) {$1$};
        \foreach \x in {1,...,4}
          \node at (8, -\x*1.2) [place] (third_\x) {$h^{2}_\x$};
        
        % Output
        \foreach \x in {1,...,3}
          \node at (12, -\x*1.2 - 0.6) [place] (fourth_\x) {$y_\x$};
        \foreach \x in {1,...,3}
          \draw [->] (fourth_\x) to (13.5, -\x*1.2 - 0.6); 

        \draw [decorate,decoration={brace,amplitude=5pt,raise=-2ex}]
          (0,1) -- (8,1) node[above,midway]{Bias};
          
        % Input -> Hidden 1
        \foreach \i in {1,...,4}
          \draw [->,dashed] (first_0) to (second_\i);
        \foreach \i in {1,...,3}
          \foreach \j in {1,...,4}
            \draw [->] (first_\i) to (second_\j);
        
        % Input -> Hidden 2
        \foreach \i in {1,...,4}
          \draw [->,dashed] (second_0) to (third_\i);
        \foreach \i in {1,...,4}
          \foreach \j in {1,...,4}
            \draw [->] (second_\i) to (third_\j);
        
        % Hidden -> Output
        \foreach \i in {1,...,3}
          \draw [->,dashed] (third_0) to (fourth_\i);
        \foreach \i in {1,...,4}
          \foreach \j in {1,...,3}
            \draw [->] (third_\i) to (fourth_\j);
        
        % Text
        \node at (0, -6) [black, ] {Input Layer};
        \node at (4, -6) [black, ] {Hidden Layer 1};
        \node at (8, -6) [black, ] {Hidden Layer 2};
        \node at (12, -6) [black, ] {Output Layer};
      \end{tikzpicture}
      \caption{Illustration of a 4-layer multilayer perceptron}
      \label{fig:illustration_deep_network}
  \end{center}
\end{figure}
 
The main reason for neural networks with multiple layers is that they can model much more \textbf{complex non-linear relationships}
~\cite{Goodfellow-et-al-2016}. than single 
neurons and single layer models are able to. Using non-linear activation functions such as ReLU \eqref{eq:relu} allow neurons to model simple 
non-linear functions, and combining these into layers allows them to model these more complex non-linear relationships.

I will denote the function computed by a neural network as $f_{\vec{\theta}}(\vec{x})$, where $\vec{\theta}$ denotes the trainable parameters of the neural
network.

\subsection{Neural networks for classification}

A dataset for classification training is given as pairs of numerical \textbf{features} and a \textbf{label}: $(\vec{x}, y)$. These features are 
the input to the network,
and the label is the target. A label cannot be directly utilised by the a neural network as they operate on numerical data. Therefore it is 
necessary to transform the data into a numerical representation.

\textbf{One-hot encoding} assigns an integer $i$ to each class and makes a vector of length $n$ (where $n$ is the 
number of classes), setting the $ith$ element to one and all other elements to zero. For example, with three classes, red, green, and blue
the one hot labels would be $[1, 0, 0]$ , $[0, 1, 0]$ and $[0, 0, 1]$. The network then has an output node corresponding to each class.
This provides no implicit ordering over the classes (a problem found in integer coding) as each output node is equally important to the 
network \cite{WhyOneHo55:online}.

The \textbf{loss} of the network is the difference between the output of the network and the correct label. Concrete details are given 
in the implementation, but loss should be high when the network is performing poorly, and low when it is performing well.

\subsection{Training the network}

Neural networks usually have their weights initialised to small random values. This means that on the first pass
the output of the network will likely be very far from the correct value. Training a neural network involves adjusting its
trainable parameters ($\vec{\theta}$) until the network reaches an acceptable loss or accuracy. This is done by computing 
the gradients of the trainable parameters with respect to the loss, and updating the parameters to move the loss towards 
a minima.

\subsubsection{Updating the weights}

Forward propagation is the flow of information through the network from input features to the output values. From these output values the loss is calculated. 
The trainable parameters of a neural network can then be updated by first finding the gradient of the loss with respect to these trainable parameters.
The process of computing these gradients is called \textbf{backpropagation} and a full derivation can be found in Appendix~\ref{backprop}. The important notation is:

\begin{itemize}
  \item $\theta$ is a vector of all the trainable parameters in the network
  \item $J(\vec{\theta})_k$ is the loss for the $kth$ sample in the training set
  \item $\nabla_{\vec{\theta}} J(\vec{\theta})_k$ is a vector of gradients of the loss with respect to all the trainable parameters 
\end{itemize}

Once the gradients for all the weights have been calculated the weights are updated. By moving the weights a small step in the direction of steepest 
negative gradient the loss should decrease as the parameters are shifting it towards a minima. This means that the model is getting 
better at modelling the training set. Taking these small steps is called \textbf{gradient descent}. The weight update rule is: 
\begin{align}
  \vec{\theta} & := \mathbf{\vec{\theta}} - \eta \sum_{k} \frac{\partial J(\vec{\theta})_k}{\partial \mathbf{\vec{\theta}}} \\
  & := \mathbf{\vec{\theta}} - \eta \sum_{k} \nabla_{\vec{\theta}} J(\vec{\theta})_k \\
  & := \mathbf{\vec{\theta}} - \eta \nabla_{\vec{\theta}} \sum_{k} J(\vec{\theta})_k \label{eq:weight}
\end{align}

$\eta$ is a \textbf{hyperparameter}. Hyperparameters are parameters of the model that are not trained but are set by the user before training.
$\eta$ is called the \textbf{learning rate} and controls how large of a step is taken each time the weight is updated. If the learning rate is 
too high the weights can ``jump" over minima, and even diverge out of a minima, but if the rate is too low it can take too long to converge,
or get stuck in a less desirable local minima. The step size is also proportional to the magnitude of the gradient, allowing the optimizer to 
take a larger step towards a minima if the gradient is steeper.

\subsubsection{Batch learning}

A neural network is generally run over multiple samples for each iteration, but not all the samples in the dataset. This is called 
mini-batch gradient descent and it is the most popular because it has stability advantages over stochastic gradient descent (one sample at a time)
and computational advantages over batch (the entire dataset every iteration). 
Stochastic gradient descent has more noise as each update is based on an individual example, causing the weights to jump around more. It also doesn't take advantage of the 
parallelisation possiblity of much hardware - it's much more efficient to send multiple samples through at once. Batch has the problem of storing the whole 
dataset in memory, and only making one update per pass through the dataset can mean the model takes longer to converge to the best parameters. Mini-batch manages to
have less noise than stochastic due to averaging over multiple samples, while also taking advantage of parallelisation and not requiring huge amounts of memory.

\subsubsection{Summary}

One complete pass through the dataset is called an \textbf{epoch}. Training involves running forward and backpropagation for a certain number of epochs to minimise the loss
on the training set and cause the model to converge to a good approximation to the real function. The pseudocode for training the network is shown below.
\begin{algorithm}
  \begin{algorithmic}[1]
    \Procedure{Training}{$i$, $\mathcal{D}$, $\eta$, \texttt{model}, \texttt{loss\char`_function}}
    \For{$i$ epochs}
    \For{mini-batch $\mathcal{M}$ in $\mathcal{D}$}
    \State \texttt{data, labels} = $\mathcal{M}$
    \State \texttt{out} = \texttt{model(data)}
    \State $\mathcal{J}$ = \texttt{loss\char`_function(out, labels)}
    \State \texttt{model}.$\vec{\theta}$ = \texttt{model}.$\vec{\theta}$ - $\eta \nabla_{\vec{\theta}} \mathcal{J}$ 
    \EndFor
    \EndFor
    \EndProcedure
  \end{algorithmic}
  \caption{Train neural network via mini-batch gradient descent} 
  \label{alg:train}
\end{algorithm}

\section{The manifold hypothesis}

The \textbf{dimensionality} of data is the number of features needed to specify the data. For example, a very popular machine learning dataset
is the MNIST dataset, containing 28x28 grayscale images of handwritten digits. The dimensionality of each datapoint is 784, as that is the
number of pixels specifying each image.

\textbf{The manifold hypothesis} suggest that high dimensional data can actually be viewed as lying on or near to a
lower dimensional manifold embedded in this higher dimensional space.

The precise definition of an n-dimensional manifold is ``a 
topological space that is locally Euclidean", i.e. there is a neighbourhood around each point on the manifold that can be describe as n-dimensional 
Euclidean space. However, it is much easier to think about with an intuitive example - imagine that all the datapoints in a 
dataset lie on piece of paper, and so can be described with two features, an $x$ and $y$ axis. If the paper is taken and scrunched up and 
twisted it now has a three dimensional shape, but the data still lies on a two-dimensional manifold and, by unscrunching the paper, can still 
be described with only two features.
\begin{figure}[H]
  \centering
  \includegraphics[width=8cm]{figs/manifold.png}
  \caption{Points on a 2-dimensional manifold embedded in 3-dimensional space}
  \label{fig:manifold}
\end{figure}

Therefore, what the manifold hypothesis is suggesting is that many features in high dimensional data are actually redundant,
and the data can be described using fewer features. Again this can be seen intuitively by the fact that the set of 784 pixel images 
that look like a recognisable digit is a very small subset of the set of 784 pixel images - if the pixel values were selected randomly from 
between 0 and 255 the picture would look like random noise, leading to the conclusion that there is a lot of redundancy in the MNIST features.
\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=\linewidth,scale=1]{figs/rand_noise.png}
    \caption{Pixel values selected randomly}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\linewidth}
    \includegraphics[width=\linewidth,scale=1]{figs/digit.png}
    \caption{MNIST digit}
  \end{subfigure}
  \caption{Demonstrating redundancy of features in MNIST dataset}
  \label{fig:digit}
\end{figure}

The manifold hypothesis then leads to \textbf{non-linear dimensionality reduction} techniques. By finding this lower dimensional
mainfold the data can be explained with fewer features and possibly be more easily separated and classified. Going back to the paper example,
linear dimensionality reduction techniques (e.g. principal component analysis) would be able to find the 2D embedding if the paper were 
rotated, translated or stretched, but would be unable to unscrunch the paper, as that amounts to a non-liner embedding. Autoencoders are 
used for non-linear dimensionality reduction, and are described in the next section.

\section{Autoencoders}

Autoencoders use neural networks in an unsupervised way to try and learn new \textbf{latent} representations of the data, typically with reduced 
dimensionality (i.e. there are fewer features in the latent data than in the input data). The use of neural networks allow it to learn a 
non-linear mapping from the data to the latent representation, the advantages of which were explained in the previous section.

Autoencoders are made up of two parts, the encoder and the decoder. The encoder takes in the original data, $\vec{x}$, 
and outputs a latent representation, $\vec{z} = f_{\vec{\theta}}(\vec{x})$. The decoder then takes in this latent representation and 
attempts to reconstruct $\vec{x}$, outputting $\vec{\hat{x}} = h_{\vec{\phi}}(\vec{z})$. This allows an unsupervised problem to be turned 
into a supervised problem, using $\vec{x}$ as the target.
\begin{figure}[H]
  \begin{center}
      \begin{tikzpicture}
        \tikzstyle{place}=[circle, draw=black, minimum size = 8mm]
        
        % Input
        \foreach \x in {1,...,6}
          \draw node at (0, -\x*1.2) [place] (first_\x) {$x_\x$};
        
        % Hidden 1
        \foreach \x in {1,...,4}
          \node at (3, -1.2 -\x*1.2) [place] (second_\x){};

        % Latent
        \foreach \x in {1,...,3}
          \node at (6, -1.8 -\x*1.2) [place] (third_\x){$z_\x$};

        % Hidden 2
        \foreach \x in {1,...,4}
          \node at (9, -1.2 -\x*1.2) [place] (fourth_\x){};
        
        % Output
        \foreach \x in {1,...,6}
          \draw node at (12, -\x*1.2) [place] (fifth_\x) {$x_\x$};
          
        \foreach \i in {1,...,6}
          \draw [->] (-1.5, -\i*1.2) to (first_\i);

        \foreach \i in {1,...,6}
          \foreach \j in {1,...,4}
            \draw [->] (first_\i) to (second_\j);

        \foreach \i in {1,...,4}
          \foreach \j in {1,...,3}
              \draw [->] (second_\i) to (third_\j);

        \foreach \i in {1,...,3}
          \foreach \j in {1,...,4}
            \draw [->] (third_\i) to (fourth_\j);
        
        \foreach \i in {1,...,4}
          \foreach \j in {1,...,6}
              \draw [->] (fourth_\i) to (fifth_\j);

        \foreach \i in {1,...,6}
          \draw [->] (fifth_\i) to (13.5, -\i*1.2);

        \draw [decorate,decoration={brace,amplitude=5pt,raise=-2ex}]
          (0,0) -- (6,0) node[above,midway]{Encoder};
        \draw [decorate,decoration={brace,amplitude=5pt,raise=-2ex}]
          (6,0) -- (12,0) node[above,midway]{Decoder};
        
        % Text
        \node at (0, -8.2) [black, ] {Input Layer};
        \node at (6, -8.2) [black, ] {Latent Layer};
        \node at (12, -8.2) [black, ] {Output Layer};
      \end{tikzpicture}
      \caption{Illustration of a simple autoencoder}
      \label{fig:illustration_autoencoder}
  \end{center}
\end{figure}

\subsection{Simple autoencoders}

Simple autoencoders, as in the figure above, constrain the network by making the number of latent features smaller than the number of input
features. This prevents the network from simply learning the identity function, and hopefully results in an informative latent space.

\subsubsection{Training}
Autoencoders can be trained end to end using backpropagation as explained in section \ref{backprop}. In order to do this they need a loss
function, and the simplest and most widely used is MSE loss \eqref{eq:mse}. The target $x$ and output $\hat{x}$ are both vectors and so 
the Euclidean distance is used as the loss per datapoint. Both the encoder weights $\theta$ and decoder weights $\phi$ are trained at the 
same time; the gradients are backpropagated through the decoder to the latent layer and then back through the encoder every iteration. 

\subsection{Denoising autoencoders}

Denoising autoencoders corrupt the input data with noise (e.g. by adding Gaussian noise or setting some of
the features to 0) giving $\tilde{\vec{x}}$, which is then used as the input to the network. However the target is the uncorrupted input data, 
$\vec{x}$. The aim is to force the autoencoder to learn a better set of features because it not only has to reconstruct the data but also 
has to remove the noise. They can be trained in the same way as a simple autoencoder.

Denoising autoencoders were used extensively to pre-train deep neural networks in the early 2010s, but this has become less popular with the 
introduction of new \textbf{non-saturating} activation functions such as ReLU which have helped deal with the vanishing gradient problem
[Appendix ??].

\subsection{Variational autoencoders} \label{vae}

Variational autoencoders are based on Bayesian inference, and differ from other autoencoders in that the encoder outputs
the parameters of a probability distribution over the latent variables, rather than a single configuration. Variational autoencoders were 
introduced by Kingma and Welling in 2013 and have become one of the most popular unsupervised learning techniques.
\begin{figure}[H]
  \begin{center}
      \begin{tikzpicture}
        \tikzstyle{place}=[circle, draw=black, minimum size = 8mm]
        
        % Input
        \foreach \x in {1,...,6}
          \draw node at (0, -\x*1.2) [place] (first_\x) {$x_\x$};
        
        % Hidden 1
        \foreach \x in {1,...,4}
          \node at (2, -1.2 -\x*1.2) [place] (second_\x){};

        % Mu
        \foreach \x in {1,...,2}
          \node at (4, -0.8 -\x*1.2) [place] (mu_\x){$\mu_\x$};

        \foreach \x in {1,...,2}
          \node at (4, -4 -\x*1.2) [place] (logvar_\x){$\sigma_\x$};

        \foreach \x in {1,...,2}
          \draw node at (6, -2.4 -\x*1.2) [place] (sample_\x){$z_\x$};

        % Hidden 2
        \foreach \x in {1,...,4}
          \node at (8, -1.2 -\x*1.2) [place] (fourth_\x){};
        
        % Output
        \foreach \x in {1,...,6}
          \draw node at (10, -\x*1.2) [place] (fifth_\x) {$x_\x$};
          
        \foreach \i in {1,...,6}
          \draw [->] (-1.5, -\i*1.2) to (first_\i);

        \foreach \i in {1,...,6}
          \foreach \j in {1,...,4}
            \draw [->] (first_\i) to (second_\j);

        \foreach \i in {1,...,4}
          \foreach \j in {1,...,2}
              \draw [->] (second_\i) to (mu_\j);
        \foreach \i in {1,...,4}
          \foreach \j in {1,...,2}
            \draw [->] (second_\i) to (logvar_\j);

        \foreach \i in {1,...,2}
          \draw [->] (logvar_\i) to (sample_\i);
        
        \foreach \i in {1,...,2}
          \draw [->] (mu_\i) to (sample_\i);

        \foreach \i in {1,...,2}
          \foreach \j in {1,...,4}
            \draw [->] (sample_\i) to (fourth_\j);
        
        \foreach \i in {1,...,4}
          \foreach \j in {1,...,6}
            \draw [->] (fourth_\i) to (fifth_\j);

        \foreach \i in {1,...,6}
          \draw [->] (fifth_\i) to (11.5, -\i*1.2);

        \draw [decorate,decoration={brace,amplitude=5pt,raise=-2ex}]
          (0,0) -- (4,0) node[above,midway]{Encoder $q_\theta(z|x)$};
        \draw [decorate,decoration={brace,amplitude=5pt,raise=-2ex}]
          (6,0) -- (10,0) node[above,midway]{Decoder $p_\phi(x|z)$};
        
        % Text
        \node at (0, -8.2) [black, ] {Input Layer};
        \node at (4, -7.8) [black, align=left] {Latent \\ Distribution\\ Parameters};
        \node at (6, -6) [black, ] {Sample};
        \node at (10, -8.2) [black, ] {Output Layer};
      \end{tikzpicture}
      \caption{Illustration of a Gaussian variational autoencoder}
      \label{fig:gauss_vae}
  \end{center}
\end{figure}

The central idea of variational autoencoders is that the data $x$ has been generated from some lower dimensional latent
representation $z$. Each datapoint $x_{i}$ is generated by:
\begin{itemize}
  \item sampling $z_{i}$ from the prior distribution over $z$: $z_{i} \sim p(z)$, 
  \item sampling $x_{i}$ from the conditional distribution $p(x|z)$ (known as the likelihood): \\ $x_{i} \sim p(x|z=z_{i})$
\end{itemize}

The prior $p(z)$ can be thought of as constraining the possible space of latent variables. For example, in the MNIST dataset a 
normal autoencoder could place 3s written in different styles in different areas of n-dimensional Euclidean space (where n is the 
dimensionality of $z$) as the latent representation is a vector of unconstrained real numbers. This is detrimental to learning a meaningful 
latent space, and by constraining this space with a prior it should force the model to keep the representations of similar datapoints close.

The decoder of the VAE is a neural network that models the likelihood, $p_\phi(x|z)$. Once the VAE is trained new samples similar to 
those in the training set can be generated by sampling from the prior and passing this into the decoder; this is why VAE's are often
referred to as generative models.

In most situations where unsupervised learning is useful only $x$ is known. The goal is then to infer $z$.
Inference in the model refers to finding good values of the latent variables given the data. This can be done by computing the
posterior, $p(z|x)$. Using Bayes rule we have:
\begin{equation}
  p(z|x) = \frac{p(x|z)p(z)}{p(x)}
\end{equation}

The denominator $p(x)$ is known as the evidence and calculating it is intractable as it has to be computed by marginalizing out $z$:
\begin{equation}
  p(x) = \int p(x|z)p(z) dz
\end{equation}

Computing this integral requires exponential time as it has to be computed over all the possible configurations of
the latent variables. Therefore the posterior is approximated with another simpler distribution $q(z|x)$ defined so that it is 
tractable. The most popular distribution, and the one used in this project, is the Gaussian.
This can then be modelled by a neural network $q_\theta(z|x)$. This is the encoder.

\subsubsection{Training}

The loss function used for a variational autoencoder can be derived by maximizing the probability of the evidence.
This makes sense intuitively, as a good model should maximise the probability of the real data ~\cite{SVIPartI90:online}.
\begingroup
\allowdisplaybreaks
\begin{align*}
  \log p(x) & = \log \int p(x|z)p(z) dz &\text{Law of total probability}\\
  & = \log \int p(x|z)p(z) \frac{q(z|x)}{q(z|x)} dz\\
  & = \log\left(\mathbb{E}_q \left[\frac{p(x|z)p(z)}{q(z|x)}\right]\right) \\
  & \geq \mathbb{E}_q \left[\log\frac{p(x|z)p(z)}{q(z|x)}\right] &\text{Jensen's inequality}\\
  & = \mathbb{E}_q \left[\log p(x|z) + \log\frac{p(z)}{q(z|x)}\right] \\
  & = \mathbb{E}_q [\log p(x|z)] - D_{KL}(q(z|x)||p(z))
\end{align*}
\endgroup

The final line of this equation is the \textbf{evidence lower bound} (ELBO). Maximizing the ELBO maximizes the probability of the 
evidence in the model, meaning the model fits the data as well as possible. The two terms in the ELBO correspond to the negative 
reconstruction loss and the \textbf{Kullback-Leibler divergence} between the computed posterior and the prior distribution of the latent variables. 
The KLD measures the difference between two probability distributions, and here measures the difference between the computed posterior and
the real prior. This acts as a regularizing term, constraining the distribution of the latent variables to be close to the prior. Taking the 
negative of the ELBO gives the loss function for the variational autoencoder. Minimizing this loss function is equivalent to maximising the ELBO.

\paragraph{The Gaussian VAE}is the most common and the one used in this project. The prior $p(z)$ is chosen to be the standard Gaussian, $\mathcal{N}(0, 1)$,
for every latent variable. The output from the encoder is a vector of means $\vec{\mu}$ and standard deviations $\vec{\sigma}$ of normal distributions. 
During training, data is fed in mini-batches into the encoder and latent variables are then sampled from the encoder output: 
$\vec{z_{i}} \sim \mathcal{N}(\vec{\mu}, \vec{\sigma}^{2})$. These are fed into the decoder which outputs a reconstruction $\vec{\hat{x}}$. The loss 
for each datapoint is then computed as the MSE loss \eqref{eq:mse} between $\vec{\hat{x}}$ and $\vec{x}$ and the KLD between
$\mathcal{N}(\vec{\mu}, \vec{\sigma}^{2})$ and $\mathcal{N}(0, 1)$.

It is not possible to backpropagate the reconstruction loss through the drawing of the random sample and so \textbf{the reparameterization trick} is used. 
An explanation can be found in Appendix ~\ref{reparam}.

\section{Semi-supervised learning}

Semi-supervised learning involves leveraging large amounts of unlabelled data to increase model performance on unsupervised learning tasks. 
In most fields unlabelled data is much easier to obtain than labelled data. For example in the field of computer vision it is very easy to 
find huge amounts of unlabelled data on the iternet, but finding labelled data can involve hiring human annotators to label the images, an 
expensive and time consuming process.

In the field of genomics there are often cases in which data that is generated in different labs can be usefully combined in machine learning
models (e.g. The Cancer Genome Atlas). However, the different labs often measure different characteristics of the organisms, and so only a 
small subset of this combined dataset may have the labels required for a task. In supervised learning the rest of the dataset is now useless,
but a semi-supervised model can leverage this data.

Perhaps the simplest semi-supervised learning method is \textbf{self-training}. This involves training a classification model on the small
amount of available labelled data. This model is then run on the unlabelled data, and the datapoints for which the model is most confident
of the label are added to the labelled dataset. The labelled dataset then becomes larger with (hopefully) mostly correct labels giving the 
model has a larger training dataset. The methods used in this project are described below, with more in-depth explanations in the 
implementation section.

\subsection{Dimensionality reduction}

The simplest semi-supervised model in this project relies on the manifold hypothesis. It uses a variational autoencoder to construct a
reduced dimnesionality feature representation of the data that should cluster similar samples together. The idea is that it should then be 
easier to classify the datapoints, even with a limited amount of labelled data. The classifier used in this project is a neural network 
with softmax outputs. This differs from the implementation used by Kingma and Welling which uses a transductive support vector machine, 
and is done for simplicity of the project.

The model has to be trained in two stages, with the autoencoder trained on the combined labelled and unlabelled data, and the classifier
trained on the latent representation of the labelled data. Once training is complete data can be classified by first passing it through 
the encoder and then through the classifier.

While a latent representation should hopefully lead to easier classification, at least some information is usually lost during dimensionality
reduction and so often other methods are preferable.

\subsection{Network pre-training} \label{sdae}

\textbf{Stacked denoising autoencoders} are a way of pre-training deep networks one layer at a time. Each hidden layer in the network is 
trained as part of a one layer denoising autoencoder, with the weights of the layer to be trained as the encoder and a new temporary layer
as the decoder. The autoencoder takes the ouput from the previous layer in the network and uses this as the input, injecting noise before 
passing it through the autoencoder and attempting to reconstruct the clean input. The reconstruction loss is then backpropagated throught 
the autoencoder only (no other layers of the deep network) and the weights are updated. The loss computed by the autoencoder is referred to 
as an unsupervised  "local denoising criterion"~\cite{Vincent:2010:SDA:1756006.1953039} as it does not require a label and is computed only 
for one layer at a time rather than the whole network. This allows the large amount of unlabelled data to be leveraged in training each layer.

The network is trained greedily, beginning with the first hidden layer. Once the reconstruction loss for the denoising autoencoder has 
converged for the layer the decoder is discarded, and the next layer is trained in the same way, using the output of the previously trained
layer as input.

Once this unsupervised pre-training is finished the model is then \textit{fine-tuned} by running normal supervised training, backpropagating
classification loss through the entire network and updating the weights.

The idea behind the unsupervised pre-training with a stacked denoising autoencoder is that it provides a good prior to the supervised training.
The pre-training procedure provides an initialization point for the supervised training where the parameters are restricted, hopefully to an area 
closer to the global minimum for the loss function~\cite{Erhan:2010:WUP:1756006.1756025}.

\subsection{The semi-supervised VAE} \label{ssVAE}

The semi-supervised VAE is a model due to Kingma et al.~\cite{DBLP:journals/corr/KingmaRMW14} that extends the VAE to include label information. 
The assumption used is that the data $\vec{x}$ is generated from both a discrete label $\vec{y}$ and a continuous latent representation 
$\vec{z}$, which are marginally independent of each other.
Therefore $\vec{y}$ encodes the class of the data, while $\vec{z}$ encodes everything else. In the MNIST dataset this means that $\vec{y}$ encodes
what digit the character is, while $\vec{z}$ encodes the style. The generative model (the decoder) works by sampling $\vec{y}$ from a 
categorical distribution $p(y)$ and by sampling $\vec{z}$ from a continuous distribution $p(z)$ (usually a Gaussian), before computing 
$p(x|y, z)$ using a neural network $p_{\phi}(x|y, z)$.
\begin{figure}[H]
  \centering
  \begin{subfigure}[b]{0.4\linewidth}
    \centering
    \begin{tikzpicture}
      \draw node at (0, 0) [place] (z) {$\vec{z}$};
      \draw node at (2, 0) [place] (y) {$\vec{y}$};
      \draw node at (1, -2) [place] (x) {$\vec{x}$};

      \begin{scope}[on background layer]
        \fill[fill=gray!25] (x.0) arc [start angle=0, end angle=360, radius=4mm];
        \fill[fill=gray!25] (y.270) arc [start angle=270, end angle=450, radius=4mm];
      \end{scope}

      \draw [->] (z) to (x);
      \draw [->] (y) to (x);
    \end{tikzpicture}
    \caption{The generative model}
  \end{subfigure}
  \begin{subfigure}[b]{0.4\linewidth}
    \centering
    \begin{tikzpicture}
      \draw node at (0, 0) [place] (z) {$\vec{z}$};
      \draw node at (2, 0) [place] (y) {$\vec{y}$};
      \draw node at (1, -2) [place] (x) {$\vec{x}$};

      \begin{scope}[on background layer]
        \fill[fill=gray!25] (x.0) arc [start angle=0, end angle=360, radius=4mm];
        \fill[fill=gray!25] (y.270) arc [start angle=270, end angle=450, radius=4mm];
      \end{scope}

      \draw [->] (x) to (y);
      \draw [->] (x) to (z);
      \draw [->] (y) to (z);
    \end{tikzpicture}
    \caption{The inference model}
  \end{subfigure}
  \caption[Semi-supervised VAE]{The semi-supervised VAE \\ (shading indicates whether a variable is found in the dataset)}
  \label{fig:digit}
\end{figure}

In the semi-supervised model there are two inference cases. When the data is labelled the question is of how to infer $z$ from $x$ and $y$.
Despite $x$ and $y$ being marginally independent, they are not necessarily conditionally independent, and so both are used as input to
the encoder. Another reason for this is that it helps the encoder to learn to separate the representation of $y$ and $z$, so that $z$ contains
no information about the label. The semi-supervised VAE in this project uses a Gaussian distribution as the tractable distribution for 
$z$ (like the unsupervised VAE), and the encoder models $q(z|x, y)$ with a network $q_{\theta}(z|x, y)$. For labelled data the model should 
maximise the evidence $p(x, y)$, the probability the model assigns to the real data. This leads to a variant of the ELBO by marginalizing
out $z$:
\begingroup
\allowdisplaybreaks
\begin{align*}
  \log p(x, y) & = \log \int p(x, y, z) dz \\
  & = \log \int p(x|y, z)p(y)p(z) dz \qquad \text{Independence of $z$ and $y$}\\
  & = \log \int p(x|y, z)p(y)p(z) \frac{q(z|x, y)}{q(z|x, y)} dz\\
  & = \log\left(\mathbb{E}_q \left[\frac{p(x|y, z)p(y)p(z)}{q(z|x, y)}\right]\right) \\
  & \geq \mathbb{E}_q \left[\log\frac{p(x|y, z)p(y)p(z)}{q(z|x, y)}\right] \\
  & = \mathbb{E}_q \left[\log p(x|y, z) + \log p(y) + \log\frac{p(z)}{q(z|x, y)}\right] \\
  & = \mathbb{E}_q [\log p(x|y, z) + \log p(y)] - D_{KL}(q(z|x, y)||p(z)) \\
  & = -\mathcal{L}(x, y)
\end{align*}
\endgroup

This is very similar to the ELBO for the normal VAE, except that there is now a prior over $y$. This encodes previous knowledge about the 
distribution of the classes $y$, penalizing the model more when the label is of a low probability class. This is unimportant for the labelled
data as the previous knowledge is drawn from this data, but becomes important for the unlabelled data, and the connection between the two can be 
seen in the derivation below. A good model maximises the ELBO and therefore minimises $\mathcal{L}(x, y)$, which is used as the loss function.

When the data is unlabelled the problem is of inferring both $y$ and $z$ from $x$, $q(y, z|x)$. The evidence in the unlabelled case is $p(x)$,
and the ELBO can be derived by marginalizing out both $y$ and $z$:
\begingroup
\allowdisplaybreaks
\begin{align*}
  \log p(x) & = \log \sum_{y} \int p(x, y, z) dz \\
  & = \log \sum_{y} \int p(x, y, z) \frac{q(z, y|x)}{q(z, y|x)} dz\\
  & = \log \sum_{y} \int p(x, y, z) \frac{q(z|x, y)q(y|x)}{q(z|x, y)q(y|x)} dz\\
  & \geq \sum_{y} q(y|x) \int q(z|x, y) \log\frac{p(x, y, z)}{q(z|x, y)q(y|x)} dz \qquad \text{Jensen's inequality}\\
  & = \sum_{y} q(y|x) \int q(z|x, y) \log\frac{p(x, y, z)}{q(z|x, y)} dz - \sum_{y} q(y|x) \log q(y|x) \int q(z|x, y) dz\\
  & = \sum_{y} q(y|x)(-\mathcal{L}(x, y)) + \mathcal{H}(q(y|x)) \\
  & = -\mathcal{U}(x, y)
\end{align*}
\endgroup

The marginalization of $y$ is done by summation because the labels are discrete. Looking at the penultimate line of the derivation there is
the term $q(y|x)$. This is classification, inferring $y$ from $x$. This classifier is parameterised
by a neural network, and outputs a categorical distribution over the labels using the softmax function \eqref{softmax}. The summation term 
this is part of is referred to as ``classification as inference'' by Kingma~\cite{DBLP:journals/corr/KingmaRMW14}. For each label the labelled loss with 
respect to the data and that label is calculated and then multiplied by the probability of the label. This means that if a particular label
leads to a bad reconstruction, implying that the label was incorrect, and the classifier assigns a high probability to that (likely incorrect) label, 
the loss to the classifier will be very high, and minimizing this loss should lead to better classification. The prior over $y$ in $\mathcal{L}(x, y)$
becomes important here as it discourages the classifier from assigning a high probability to an unlikely class too often. All of this means that 
the classifier can learn directly from unlabelled data, with the small amount of labelled data providing a guide to good reconstructions.

The pipeline for labelled and unlabelled data is therefore slightly different at the moment. Labelled data is fed directly into the encoder
along with its label, and the loss function $\mathcal{L}(x, y)$ is calculated and backpropagated through the network. Unlabelled data is 
first put through the classifier, before it is fed into the encoder once with each label allowing $\mathcal{U}(x)$ to be computed and 
backpropagated through the network.

This means that at no point does the classifier learn directly from the labelled data, unnecessarily disadvantaging the model. To remedy
this the labelled loss function is modified to include an extra term, the cross entropy loss \eqref{eq:ce} between the real label and the 
label the classifier outputs for the data. This modified version is then:
\begin{align}
  \mathcal{J}(x, y) = \mathcal{L}(x, y) - \alpha (y \cdot \log q(y|x))
\end{align}

$\alpha$ is a hyperparameter that controls the weighting of the supervised loss, and is configured depending on the amount of labelled and 
unlabelled data available.

The model can now learn to classify from both labelled and unlabelled data at the same time, and so does not require pretraining the way
the previous models did.

\subsection{The ladder network} \label{ladder}

The ladder network is the most recent of the models included in this project having first been described by Valpola in 2014
~\cite{DBLP:journals/corr/Valpola14}, and then expanded upon by Rasmus et al. in 2015~\cite{DBLP:journals/corr/RasmusVHBR15}. 
It can be thought of as building on top of the SDAE, using an unsupervised local denoising 
criterion in order to improve the supervised performance of a deep feedforward network by using unlabelled data. However, instead of 
training each layer with its own denoising autoencoder, it instead adds a single deep decoder of the same size as the supervised network 
to the model, with lateral connections with trainable weights between the equivalent layers in the encoder and decoder.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.25]{figs/ladder.png}
  \caption[Illustration of the ladder network]{Illustration of the ladder network \\ (Source: Rasmus et al.~\cite{DBLP:journals/corr/RasmusVHBR15})}
  \label{fig:ladder}
\end{figure}

As described by Valpola, the ladder network is a form of hierarchical latent variable model. These differ from the latent variable models
looked at so far (autoencoder, VAE) in that they attempt to represent the data using hierarchies of latent variables. The model contains
layers of latent variables, with the higher layers describing \textit{some} of the variance of lower layers by $p(z^{(l)}|z^{(l+1)})$ ($z^{(0)} = x$). 
In a normal autoencoder the single latent layer $z$ has to encode everything about the data $x$, otherwise the reconstruction it generates 
will be very poor. With a hierarchical model each layer models some information about $x$, with the higher level latent variables 
(further from the data) able to be more abstract, as they don't have to encode the lower level details encoded in the previous (lower) layers. For example, using MNIST, $z$ in a simple
autoencoder cannot just be the digit label as this does not provide enough information to reconstruct the original image well. In a hierarchical latent variable model the highest
level latent variable is able to be very abstract and only encode the label, as the other layers in the model provide less abstract information that can lead to a good reconstruction.
The reason this works in the ladder network is due to the lateral connections, which "leak" information from layers in the encoder to the decoder. This means that each decoder layer
receives information both from the previous decoder layer and the corresponding encoder layer. The encoder layer passes some information which
the subsequent encoder layers then no longer have to model.

This structure with the representation becoming more abstract in higher level layers is also analagous to how supervised learning works, with the layers 
further into the network modelling more complicated and abstract features with the final layer just outputting a class label (most abstract). For example in convolutional
neural networks (used extensively in vision), visualisation of the feature maps learned in image classification tasks have shown they are hierarchical. The lower layers
have high spatial resolution for detecting low-level features like edges and the higher layers have lower spatial resolution but can capture more abstract and complex features
(like people)~\cite{conv_hierarch}.

At the input to each decoder layer $u^{(l)}$ the corresponding encoder representation $z^{(l)}$ and the output of the previous decoder layer $u^{(l+1)}$ are combined together
using a combinator function $g$. $g$ has trainable parameters, but this leads to a problem where the lowest possible unsupervised MSE loss \eqref{eq:mse} can be 
achieved by $g$ learning to copy $z^{(l)}$ and completely ignoring the $u^{(l+1)}$, which corresponds to copying the input directly to the output at the bottom layer.
This completely short circuits the autoencoder, and in order to prevent this, Gaussian noise (sampled from $\mathcal{N}(0, 1))$) is added to the input to each layer 
in the encoder. Each encoder layer then has the representation $\tilde{z}^{(l)}$ and this noise means that just copying over the input no longer minimises the cost function. 

However if the unsupervised cost function used is simply the reconstruction loss between $\hat{x}$ and $x$ the first layer of the network still has a disproportionate
influence on the loss. In order to remedy this Valpola proposes adding local denoising criterion. This involves adding a cost function at each layer of the decoder,
namely the MSE~\eqref{eq:mse} between $g(\tilde{z}^{(l)}, u^{(l+1)})$, which is $\hat{z}^{(l)}$, a denoised representation of $\tilde{z}^{(l)}$, and $z^{(l)}$, the clean 
representation from layer $l$ of the encoder. In order to generate both the clean and noisy encoder representations the encoder is run twice per training iteration,
once without the added noise to generate $z^{(l)}$, and once with noise to generate $\tilde{z}^{(l)}$. These local cost functions require all the layers to learn in order
to make a meaningful representation that can be denoised well. Each layer loss is multiplied by a hyperparameter $\lambda_{l}$ according to how important the denoising cost
of the layer is, before being summed together to give a final unsupervised cost function per sample of:

\begin{align}
  \mathcal{U}(x) = \sum_{l} |z^{(l)} - \hat{z}^{(l)}|^{2}
\end{align}

The model as explained so far allows the ladder network to learn abstract features in the higher layers. However, without supervised data and a supervised cost function the 
features learned are unlikely to be useful for the classification task. The small amount of labelled data is used as a guide for this. The encoder of the model is the 
original classifier so the data is passed through the encoder and cross entropy loss~\eqref{eq:ce} is computed between the output and the labels. During training noise is 
added at the output of each layer, the same as one run of supervised learning, as the noise acts as a regularizer~\cite{NoiseInj}.

While the illustration of the model (Fig.~\ref{fig:ladder}) makes it look like it has three networks, the first and third column are both the encoder, with the first being 
noisy and the second being clean. A final overview of the training process is then:
\begin{enumerate}
  \item Data is passed into the clean encoder and the output from each layer is saved.
  \item Data is passed through the noisy encoder, and output from each layer is saved.
  \item The final output from the noisy encoder is the classification output, and supervised loss is computed with it.
  \item The output from the noisy encoder is fed into the decoder. At each layer $l$ of the decoder $g(\tilde{z}^{(l)}, u^{(l+1)})$ is calculated,
        and the unsupervised loss for the layer is computed.
  \item The unsupervised losses are multiplied by hyperparameter $\lambda_{l}$ and summed together.
  \item The supervised and unsupervised losses are summed and backpropagated through the network and the weights are updated.
\end{enumerate}  

\section{Requirements analysis}

By taking the success criteria from the project proposal (Appendix ~\ref{proposal}) I constructed a set of tasks that must be completed for the 
project and ranked them by their priority. The success criteria are:

\begin{itemize}
  \item The implemented models achieve close to original paper performance on the MNIST dataset 
  \item The final chosen model achieves better prediction accuracy than supervised learning alone on genetic datasets
  \item A tool is built that takes in file paths to unlabelled and labelled data and trains a classifier based on this
\end{itemize}

The first success criteria differs from the project proposal. After reading the papers of the models to be implemented (Section~\ref{reading}) I 
realised that the most important models, the semi-supervised variational autoencoder and the ladder network, both had benchmarks given on 
the MNIST database of handwritten digits~\cite{lecun-mnisthandwrittendigit-2010}. As the stated aim of generating synthetic data was to
ensure that the models were working correctly, I decided that a better measure of the correctness of the models was whether they
achieved (close to) the accuracy reported in the papers.

The models compared in this project in order to decide on the best semi-supervised method to use were:

\begin{itemize}
  \item \textbf{A variant of the Kingma M1 model}~\cite{DBLP:journals/corr/KingmaRMW14}. A variational autoencoder trained on combined 
        labelled and unlabelled data followed by a neural network classifier trained on the latent representation of the labelled data.
  \item \textbf{A stacked denoising autoencoder}~\cite{Vincent:2010:SDA:1756006.1953039}.
  \item \textbf{A semi-supervised variational autoencoder}.~\cite{DBLP:journals/corr/KingmaRMW14} Also referred to as the Kingma M2 model.
  \item \textbf{A ladder network}~\cite{DBLP:journals/corr/RasmusVHBR15}
\end{itemize}

With these models selected and the success criteria defined above the requirements can be constructed:

\begin{table}[H]
  \label{tab:requirements}
  \small % text size of table content
  \centering % center the table
  \begin{tabular}{cc} % alignment of each column data
  \toprule[\heavyrulewidth]\toprule[\heavyrulewidth]
  \textbf{Requirement} & \textbf{Priority} \\ 
  \midrule
  Implement simple multilayer perceptron & Medium \\
  Implement Kingma M1 model & Medium \\
  Implement stacked denoising autoencoder & Medium \\
  Implement semi-supervised autoencoder & High \\
  Semi-supervised autoencoder achieves close to original paper \\ accuracy on MNIST & Medium \\
  Implement ladder network & High \\
  Ladder network achieves close to original paper accuracy \\ on MNIST & Medium \\
  Process the Cancer Genome Atlas gene expression data & High \\
  Evaluate and compare performance of models on MNIST & Low \\
  Evaluate and compare performance of models on TCGA data & High \\
  Implement saliency for best performing model (extension) & Low \\
  \bottomrule[\heavyrulewidth] 
  \end{tabular}
  \caption{Requirements for a successful project} 
\end{table}

\section{Starting point and reading} \label{reading}

I had some previous knowledge about neural networks and machine learning techniques through completing \textit{Introduction to data science}
and \textit{Artificial intelligence I} as part of Part IB, and completing the machine learning course by Andrew Ng on Coursera. However,
I had no previous experience with autoencoders, semi-supervised learning or Bayesian inference, and also little experience of optimising 
a machine learning model. To this end I decided upon a list of essential reading:

\begin{itemize}
  \item Lecture slides for \textit{Machine learning and Bayesian inference} by Holden~\cite{ML_Bayes}
  \item \textit{Deep learning} by Goodfellow et al.~\cite{Goodfellow-et-al-2016}
  \item \textit{Auto-encoding variational Bayes} by Kingma et al.~\cite{DBLP:journals/corr/KingmaW13}
  \item \textit{Semi-supervised learning with deep generative models} by Kingma et al.~\cite{DBLP:journals/corr/KingmaRMW14}
  \item \textit{Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion} by 
        Vincent et al.~\cite{Vincent:2010:SDA:1756006.1953039}
  \item \textit{From neural PCA to deep unsupervised learning} by Valpola~\cite{DBLP:journals/corr/Valpola14}
  \item \textit{Semi-supervised learning with ladder networks} by Rasmus et al.~\cite{DBLP:journals/corr/RasmusVHBR15}
\end{itemize}

\section{Resources}

\subsection{Language and libraries}
I made the decision to use Python, as it has support for two of the most popular and intuitive deep learning libraries, PyTorch and 
Tensorflow. Both frameworks are computationally similar, using a tape-based automatic differentiation system for backpropagation, and a C/C++ 
engine for speed. After reading through an introduction to both languages, and looking at a comparison of the features offered I decided to 
use PyTorch, as it felt more flexible, allowing the user to easily stop and start training at any point, and allowing much easier access 
to intermediate variables for debugging. In Tensorflow all operations have to be run using a Session object, and the only variables a 
user can see are those returned from the session object.

I used the PyCharm IDE for writing my Python code as I was familiar with Jetbrains IDEs.
PyCharm includes useful features like autocomplete, and has good integration with git.

\subsection{Hardware}
All of the programming was done on my laptop (Macbook Pro 2014, 2.6 GHz Intel Core i5, with 8GB of RAM, a 128GB SSD and 128GB SD card).

Running the models, especially on the highly dimensional gene expression data, is extremely computationally intensive, and runs much 
quicker on a GPU. To this end I was given access to NVIDIA Titan X and P100 GPUs by Prof. Li\'o.

\subsection{Backing up}
To back up my project and dissertation files and code I stored them in a git repository synced with a remote repository on GitHub. I also 
made regular backups of my entire SSD to an external hard drive.

\section{Summary}
This section should have provided an overview of the theory behind this project, and of the requirements that must be completed to 
ensure the project is a success.

\chapter{Implementation}

\textit{This section of the dissertation is focused on the steps involved in constructing, training and evaluating the models used in this project.
All the models were written from scratch in PyTorch, using the original papers and their implementations (in Theano) as starting points.
This section also covers the tuning of the model hyperparameters, and the subsequent evaluation of the models.}

\section{Model and class structure}

\subsection{PyTorch data structures}
The most important computational unit in PyTorch is the \textbf{tensor}. Tensors are similar to multidimensional arrays, but they are able to 
utilise GPUs to perform operations in parallel. This is important because the running of a neural network involves thousands of matrix 
operations, many of which can be performed in parallel, as they don't depend on each other. GPUs have high memory bandwidth and are 
optimised to perform simple operations in parallel, making them well suited for machine learning applications.

Tensors have the property \texttt{requires\_grad} which tells PyTorch whether to record the operations that happen to the tensor. If 
this is true the tensor records all operations on it, and build up a \textbf{computation graph}. The computation graph is a 
directed acyclic graph containing input tensors as the roots and output
tensors as leaves and has \texttt{Functions} as internal nodes. These functions are actually the expressions that are applied to the 
tensors during the forward pass. The construction of this graph allows for backpropagation to be computed easily by performing a 
backwards pass on this graph to compute the gradients. The graph is created during the forward pass, which is why they are referred to 
as \textbf{dynamic} computation graphs.

Parameters are tensors that are trainable parameters of the network, for example the weights and biases. The parameter class is a 
wrapper that tells a \textbf{module} whether to include a tensor in the parameters object of the module.

Modules are the base class for all models in PyTorch, and are combinations of parameters and functions that will be applied to input 
data when the \texttt{forward} method of the module is called. All models subclass \texttt{nn.Module} and these modules can be nested
to allow more complicated models to be constructed from basic constituent parts. For example the \texttt{nn.Linear} module contains 
weight and bias parameters, and multiplies the input data by the weights and adds the bias in the forward method.

\subsection{My base modules}
My codebase contains four base modules that are used to construct the more complicated higher level models. The simplest of 
these base classes are Classifier, Encoder and Decoder, and the difference between them is mainly semantic and used to keep the
code clearer.

The Classifier constructor takes as input the dimensionality of the input data, a list of hidden layer sizes, and the number of classes 
in the output, and constructs a simple multilayer perceptron module using \texttt{nn.Linear} for each layer. It uses ReLU as the activation 
function in all the hidden layers,
because it is non-saturating, meaning that it does not suffer from the vanishing gradients problem, where the gradients of the early layers
of the neural network become very small and so update very slowly. It uses a linear function on the output because the implementation of 
cross entropy loss in PyTorch computes log softmax inside it, and so computing softmax in the classifier is not necessary.

The Encoder is very similar to the classifier, except the constructor also takes in an activation function that is applied to the output 
of the network. This becomes important when the encoder is used in the SDAE. The Decoder constructor is the same as the Encoder to make 
constructing Encoder/Decoder pairs as simple as possible. It takes in the same inputs but reverses them to create the complementary decoder,
also including an output function if required.

The Variational Encoder is a separate module from the Encoder, because the forward pass has three outputs. 
The output of the network is two vectors of the same dimension, $\mu$ and $\sigma$, the parameters of the posterior Gaussian distributions.
It also contains a module that samples from these distributions to create the hidden vector $z$ using the reparameterization trick 
(Appendix~\ref{reparam}). The output of calling \texttt{forward} on the Variational Encoder is then $z$, $\mu$ and $\sigma$.

\subsubsection{Weight initialisation}
Random initialisation is required in neural networks to give them the best chance to find minima of the loss, as the starting 
point can greatly effect the gradients in the network and the direction the parameters move in. If the weights were 
deterministically initialised the network would always find the same minima, which may not be optimal.
Weight initialisation can help models converge more easily as reported by Glorot and Bengio~\cite{DBLP:journals/jmlr/GlorotB10}.
The default weight initialisation in PyTorch is Xavier initialisation which involves drawing the weights and biases from a normal 
distribution with the parameters below:
\begin{align*}
  \mu = 0 \qquad \sigma = \frac{1}{\sqrt{n_{l-1}}}
\end{align*}
where $n_{l-1}$ is the number of neurons in the previous layer

\subsection{Class structure}

The models used in this project all required certain methods to allow the scripts used for running the training and evaluation to be as 
model agnostic as possible. To this end I implemented a base class \texttt{Model} for all the models to ensure they all had the same methods.
\texttt{Model} also subclassed \texttt{nn.Module} to ensure that the models adhered to PyTorch standards.
\begin{figure}[H]
  \centering
  \includegraphics[scale=1]{figs/model_class.pdf}
  \caption{Model base class}
\end{figure}

The \texttt{dataset\_name} field is used to save the model state in the correct place when using early stopping, and the \texttt{device} field is 
used to ensure that the GPU is used if available, and all new tensors created are placed on the GPU. Not placing the tensors on the correct device 
leads to errors as PyTorch cannot perform operations on tensors located on different devices.

Each model has different fields depending on the structure of the model, so initialisation of the models could not be a general method. 
However, for evaluation I implemented a \texttt{hyperparameter\_loop} method per model 
which took the same arguments for all models:
\begin{center}
  \texttt{hyperparameter\_loop(fold, validation\_fold, state\_path, results\_path, dataset\_name, dataloaders, input\_size, num\_classes, max\_epochs, device)}
\end{center}

This method loops through a pre-selected set of hyperparameters to determine the best performing.
This was possible because the hyperparameters chosen for searching over were not dependent on the input dataset, and were instead selected 
to give good general performance e.g. the ladder network hyperparameter search constructs a ladder network with 1-4 hidden layers, with the 
deeper model winning on more complicated datasets and the shallower on less complicated.

\subsection{Multilayer perceptron}
The multilayer perceptron is a fully supervised model implemented to compare the performance between a fully supervised approach and the 
semi-supervised approaches used in the project. The model simply uses the Classifier base module and trains it on only the labelled data 
using the Adam optimizer for gradient descent.

\subsection{M1}
The M1 model is partially taken from the Kingma paper~\cite{DBLP:journals/corr/KingmaRMW14}. It is made up of a VAE Encoder,
Decoder and Classifier 
module. The VAE Encoder and Decoder are initially trained in an unsupervised way using both the labelled and unlabelled data. In order 
to train them I made the decision to normalise all the data to the range [0-1] and to use a Sigmoid function 
\footnote{the sigmoid function is of
the form $f(x) = \frac{1}{1+e^{-x}}$ and has the domain of the real numbers and range [0-1]} on the output of the Decoder, to ensure the output range 
was the same as the input. This was done for numerical stability reasons, as it was found that using unnormalised or standardised data 
as input to the VAE would cause the variance of the latent distribution to become infinitely small and eventually lead to the loss and 
output of the VAE becoming \texttt{nan}.

Once the VAE Encoder and Decoder are trained the Decoder is discarded. The labelled data is passed through the VAE Encoder, and the samples
from the distributions are used as input to the Classifier. The Classifier is then trained using this labelled data, using cross entropy 
loss as the loss function. 

This model relies on the manifold hypothesis. The training of the VAE hopefully clusters similar samples together by finding a lower
dimensional manifold, allowing the Classifier to more easily separate the classes using only a small amount of labelled data.

Once the Classifier is trained the full pipeline for classifying new data involves first passing the data 
through the VAE Encoder and then through the Classifier.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \node at (0, 0) [black,] (x) {$x$};
    \node at (2, 0) [rectangle,draw,align=left,fill=blue!10] (vae_e) {Variational \\ Encoder};
    \node at (6, 0) [rectangle,draw,fill=red!10] (clas) {Classifier};
    \node at (8, 0) [black, ] (y) {$y$};
    \draw [->] (x) to (vae_e);
    \draw [->] (vae_e) to node[above] {z} (clas);
    \draw [->] (clas) to (y);
  \end{tikzpicture}
  \caption{The classification pipeline of the trained M1 model}
\end{figure}

\subsection{Stacked denoising autoencoder}
The SDAE was constructed by first constructing a list of Encoders. Each Encoder had the same input size as the latent dimension of the 
previous Encoder. All the Encoders use ReLU as the output activation function, and the final output layer is a simple \texttt{nn.Linear} layer with 
no output activation. This means the structure of the network is very similar to the Classifier, but it enables easy unsupervised pretraining.

The unsupervised pretraining is performed by taking each encoder, starting with the first hidden layer, and adding the corresponding 
Decoder. This is where the pairing of the Encoder and Decoder is very useful, as by extracting the layer sizes of the Encoder they 
can be passed straight into the Decoder contructor to create the complementary Decoder. Then Gaussian noise is added to the input data,
using the PyTorch function \texttt{torch.randn\_like(data)}, which creates a Gaussian noise tensor of the same size as the data that can
simply be added to the data. This Encoder/Decoder pair is then trained like a normal autoencoder using mean square error loss, and 
this is the local denoising criterion.

Once each hidden layer has been trained like this, the whole network is trained using labelled data to ensure that the hidden layers 
are detecting the right features for the task. Classification just involves passing the data through all the Encoders and then through 
the output layer.

\subsection{M2}

The M2 model is made up of a Classifier, a Variational Encoder and a Decoder. It is again taken from the Kingma paper~\cite{DBLP:journals/corr/KingmaRMW14}.
Data is first passed through the Classifier to get a prediction for the label. The loss functions used were given in Section~\ref{ssVAE}, 
and depend on whether the data is labelled or unlabelled. If the data is labelled then the data is passed into the Variational Encoder 
with the correct label, and the output of the Variational Encoder is passed into the Decoder which attempts to reconstruct the data.
If the data is unlabelled then the data and each possible label are passed into the Variational Encoder, and the reconstruction loss 
is computed for all the combinations. This seems like it would greatly increase the training time, but this can actually be done 
by creating a larger tensor with every unlabelled sample in the batch paired with each label. This operations on this larger tensor 
can be computed in parallel on the GPU so the performance hit is not as drastic as this operation might suggest. The overall 
structure of the model is shown below:

\begin{figure}[H]
  \centering
  \begin{tikzpicture}
    \node at (2, 0.25) [rectangle,draw,fill=red!10] (clas) {Classifier};
    \node at (5.5, 0) [rectangle,draw,align=left,fill=blue!10] (vae_e) {Variational \\ Encoder};
    \node at (9, 0.25) [rectangle,draw,fill=green!10] (dec) {Decoder};
    \node at (12, 0.25) [black,] (x_hat) {$\hat{x}$};
    \draw [->] (0, 0.25) to (clas);
    \draw [->] (clas) to node[above] {$y$} (4.4, 0.25);
    \draw [->] (0, -0.25) to (4.4, -0.25);
    \draw [->] (6.6, 0.25) to node[above] {$z$} (dec);
    \draw [->] (3, 0.25) to[out=45,in=135] (dec);
    \draw [->] (dec) to (x_hat);
    \draw [decorate,decoration={brace,amplitude=5pt,mirror,raise=-1ex}]
      (-0.5,0.5) -- (-0.5,-0.5) node[left,midway]{$x$};
  \end{tikzpicture}
  \caption{Model structure of M2}
\end{figure}

The main diffculties in implementing this model came from conflicting information found online, and the lack of other implementations to view. 
The original paper can be quite hard to decipher, often skipping over difficult derivations, and at one time seemingly contradicting 
itself in the space of two lines. One of the original blog posts I viewed~\cite{Semisupe95:online} did not pass the label into the 
encoder, and after corresponding with the author I found it was due to a different interpretation of a line in the paper. However,
Kingma's implementation (available at: https://github.com/dpkingma/nips14-ssl) does do this and so that is the model I followed. A 
helpful reference implementation I found is available at https://github.com/wohlert/semi-supervised-pytorch, but I believe my implementation is 
easier to read and also performs better, as I use the inbuilt PyTorch models for computing the cross entropy loss and use the Kingma way 
of computing the KLD~\cite{DBLP:journals/corr/KingmaW13}. I was able to contribute to the Wohlert repository too, finding a bug in the 
variational autoencoder code where the natural logarithm of the variance was restricted to values greater than zero (this is incorrect).

This model again requires that the data be normalised to the range [0-1] (as in M1) as otherwise the loss explodes and becomes 
\texttt{nan} as the variance of the latent distribution becomes very small.

Once the model is trained the only part used in classification is the Classifier.

\subsection{Ladder}

The ladder network doesn't make use of any of the base modules, as it requires a lot of extra parts. My original implementation 
was based on an implementation found at https://github.com/abhiskk/ladder. However, this implementation is very slow, as at one point 
tensors on the GPU are shifted onto the CPU to perform operations using numpy, which I found was unnecessary. The implementation also did 
not achieve the desired accuracy on MNIST. For this reason I based my implementation on a Tensorflow implementation written by Rinu Boney
(available at https://github.com/rinuboney/ladder), who worked on 
the 2015 paper~\cite{DBLP:journals/corr/RasmusVHBR15}. The code required many modifications to move from Tensorflow to PyTorch,
but my implementation is now the fastest and most accurate PyTorch implementation of the model I could find on the internet.

The training process is described in some detail in Section~\ref{ladder}, and I will only add some difficulties with the implementation 
here. The ladder network uses batch normalisation at every layer in the encoder and decoder, and the encoder mean and standard deviation
are also used to normalise the reconstruction. However, the forward pass batch normalisation is done by PyTorch \texttt{nn.BatchNorm1d}
modules, which handles edge cases correctly. When normalising the reconstruction I used the statistics from the batch norm, but did 
not realise that the variance in the top layer was zero in lots of places. This was caused by the edge pixels in the MNIST images all 
being entirely black, and so I discovered that the ouput of the network became \texttt{nan}. In order to debug this I added hooks to the
data tensors to report if they ever became \texttt{nan}.

\section{Hyperparameter optimisation} \label{hyper}
While the weights and biases in a network can be trained by backpropagation, there are parameters of the model that cannot be trained in 
this way, but still affect the performance of the network. These are hyperparameters and they are set before the training begins. 
Optimising these can be crucial in getting the best performance from a model.

\subsection{Grid search}
The most common method for hyperparameter optimisation, and the one used in this project, is grid search. The researcher provides several 
likely values for the hyperparameters, and the model is trained with each combination of these. A validation set is used to compare the
performance of the models, and the best set of hyperparameters is selected this way.

However, optimising hyperparameters this way results in every combination having to be run again for each additional value of each
hyperparameter used. Depending on how long it takes each model to train, this can result in hours more training time. 
Therefore, I chose to optimise over a small number of hyperparameters that I feel are most likely to affect the performance of the models.

\subsection{Number of hidden layers}
The number of layers in a model is important because it controls the capacity~\cite{Goodfellow-et-al-2016} of the neural network. If the 
capacity of the network is too high it can \textbf{overfit} to the training data, resulting in poor generalization to new examples, and if the capacity is 
too low it can \textbf{underfit}, resulting in poor fitting to the train data. Therefore choosing the number of layers is an important hyperparameter.
The universal approximation theorem states that a neural network with a single hidden layer can approximate any function, but the layer 
has to be exponentially large to give the same capacity as a multi-layer network; instead adding new layers is the preferred method, 
with the deeper layers learning more complex and relevant features to the task.

\subsection{Hidden layer size}
Larochelle et al.~\cite{DBLP:journals/jmlr/LarochelleBLL09} found that neural networks with constant layer size usually perform better than
those where the layer size increases or decreases throughout the network. When optimising the number of layers I use a constant layer size
for each hidden layer.

Bengio et al.~\cite{DBLP:series/lncs/Bengio12} found that using a first hidden layer larger than the input layer often resulted in 
better performance. However due to the high dimensionality of gene expression data (over 20,000 genes in the TCGA data) I decided against 
this. I use a constant layer size for all the hidden layers, using the same layer size for all the different models to allow for a fairer
comparison.

\subsection{Latent dimension of autoencoders}
The size of the latent dimension for an autoencoder affects the quality of the representation learned. If the dimension is too small the 
autoencoder may not be able to encode all the important features, whereas a latent dimension that is too large can give the autoencoder 
too much freedom, causing it to not cluster important samples together.

This hyperparameter is only optimisable for the M1 and M2 models, as the hidden dimensions for the autoencoder is the layer size in the SDAE,
and the number of classes in the ladder network.

\subsection{Learning rate}
The learning rate is a hyperparameter that controls how large of a step is taken each time the weights are updated. It is an often
optimised hyperparameter because if it is too large it can overshoot loss minima, resulting in worse performance, and if it is too 
small it can take a very long time for the gradient descent to converge. However, I have chosen not to search over different learning 
rates because of the additional time it would take, and the fact that I am using the Adam optimizer. The Adam optimizer keeps a 
learning rate per parameter, and updates these learning rates according to the first and second moments of the gradients with respect to each 
parameter~\cite{DBLP:journals/corr/KingmaB14}. This means that while an initial learning rate still has to be provided, it affects the 
performance much less.

\subsection{Early stopping}
The number of epochs that a learning algorithm is trained for also affect how well it performs. Not training for long enough can result in
poor peformance and underfitting, as the model has not had enough time to learn the relevant features, while training for too long can
be wasted compute time if the model is not improving, or even lead to overfitting.

Luckily, this is one of the easiest hyperparameters to optimise, and does not have to be included in a grid search. 
With the use of a validation set that the network is not trained on the performance of each model on unseen data can be measured after 
every epoch. Unseen data is necessary because evaluating a model on training data will give overfitted models excellent performance, 
while the actual model generalises poorly and is unusable.
If the performance has improved the state of the model is saved. After a certain number of epochs without the performance improving 
the training is stopped, and the best performing model state is loaded.

\section{Data processing}
An important part of a machine learning project is the pre-processing of the data. This is used to ensure good model performance and 
to partition the data to allow unbiased evaluation.

\subsection{Datasets}
\subsubsection{MNIST handwritten digit database}
The MNIST dataset is one of the most popular in machine learning, being used to benchmark new models is many papers.
\begin{table}[H]
  \label{tab:mnist}
  \small % text size of table content
  \centering % center the table
  \begin{tabular}{lccr} % alignment of each column data
  \toprule[\heavyrulewidth]
  Samples & 60,000 train \& 10,000 test \\
  Inputs & 28x28 b/w images  \\
  Number of classes & 10 - digits 0-9 \\
  Balanced & Yes \\
  \bottomrule[\heavyrulewidth] 
  \end{tabular}
  \caption{MNIST dataset} 
\end{table}

In this project it is again used for benchmarking, and for ensuring that the models are performing similarly to their original implementations.
It does have several similarities with gene expression data though, being highly dimensional, but with only a few features being relevant to 
classification. In general, only a very few pixels are actually needed to distinguish between the different digits (and most pixels near the 
edge of the image are completely useless, being black in almost all the images), while in gene expression it is usually only a few genes 
that control each phenotype.

\subsubsection{The Cancer Genome Atlas}
The Cancer Genome Atlas is a project cataloguing sequencing data for several different types of cancer. The data generated by the TCGA Research 
Network (https://www.cancer.gov/tcga) is available online at https://portal.gdc.cancer.gov. In this project I used data generated using 
RNA-Seq to attempt to classify the different types of cancer using only their gene expression.
\begin{table}[H]
  \label{tab:tcga}
  \small % text size of table content
  \centering % center the table
  \begin{tabular}{cc} % alignment of each column data
  \toprule[\heavyrulewidth]
  Samples & 11,060 \\
  Inputs & 20,350 genes with values given as $\log_{2}(\text{TPM}+1)$ \footnotemark \\
  Number of classes & 33 - different cancer types\\
  Balanced & No \\
  \bottomrule[\heavyrulewidth] 
  \end{tabular}
  \caption{MNIST dataset} 
\end{table}
\footnotetext{TPM is transcripts per million, where the total number of reads mapped to a gene is normalised by the length of the gene} 

Some of the samples in this dataset are missing gene expression values, and these samples are not used for comparing semi-supervised model
performance. However imputation of missing data is an important topic in bioinformatics and so I cover this in Section~\ref{imput}.

\subsection{Data normalisation}
Data normalisation can help neural networks by allowing gradient descent to reach a minima more easily. Features having large ranges
can result in larger gradients of the loss, resulting in larger steps being taken at the weights. This can cause the 
gradient descent to oscillate around minima and take far longer to reach a good value. The two most common forms of data normalisation 
are standardisation and normalisation to the range [0-1]. Having all the features with similar scales is also important for saliency computation,
as different scales result in different gradients of the output with respect to the input, preventing them from being directly comparable.

\begin{itemize}
  \item \textbf{Standardisation} involves scaling all the features so that they have mean 0 and variance 1. This is done be computing the mean 
          and standard deviation of the features from the training set, and then subtracting the mean from each feature and dividing by the
          standard deviation.
  \item \textbf{Normalisation} into the range [0-1] is done by finding the maximum and the minimum for every feature from the training set,
          subtracting the minimum from every feature and dividing by the maximum minus the minimum.
\end{itemize}

For the MNIST dataset I used normalisation, as the pixels take values between 0 and 255 and so scaling this to be between 0 and 1 is 
a simple and intuitive transform. It is also the transform used in the code for the ladder network paper~\cite{DBLP:journals/corr/RasmusVHBR15}, 
and similar to the transform used in the semi-supervised VAE paper~\cite{DBLP:journals/corr/KingmaRMW14} (here they set pixel values to 
either zero or one, without the range inbetween).

For the gene expression datasets I tried both standardisation and normalisation. Standardisation worked well for the non-VAE based models,
but the M1 and M2 models experienced significant numeric instability, resulting in the variances of the normal distribution in the latent dimension 
becoming very small, which resulted in the KLD loss exploding.
This eventually lead to the output of the autoencoder becoming \texttt{nan}. From what I could discern this is a somewhat common problem in
VAEs, especially when the dimensionality of the data is higher than the number of samples in the dataset. The VAEs responded well to normalised
data, while the other models experienced similar or worse performance. Therefore standardisation was used for the MLP, SDAE and ladder 
network and normalisation was used for M1 and M2.

\subsection{Data imputation} \label{imput}
The TCGA dataset contains the gene expression levels for 20,350 genes, but in many of the samples some of these genes are missing.
This can be dealt with by simply discarding the samples with missing genes, but this can be a significant number of samples, and removing 
a large chunk of the dataset is not a good way to improve a model. Instead it may be possible to impute the missing genes, or drop the 
genes entirely, reducing the number of features but keeping the number of samples. In the evaluation section there is a brief comparison 
of the performance of dropping the genes, replacing the missing values with the feature mean, and replacing the missing values with zero.

However, to remove any effect of the imputation on the semi-supervised comparisons I removed the samples containing missing genes, leaving 
9310 samples. 

\subsection{Data partitioning}
\subsubsection{Evaluating models and hyperparameter optimisation}
Evaluating the performance of a machine learning algorithm should be done on a test dataset that is separate from the dataset the 
algorithm is trained on, to prevent overfitting leading to overestimating the performance of the model. If the dataset is not particularly 
large this can be done using 
\textbf{$k$-fold cross validation}, where the data is split into $k$ different partitions. $k-1$ of these partitions are used for training, 
while the $kth$ is used to test the performance after training. The accuracy for the model is then computed by taking the average of all
the accuracies for each fold.

\begin{figure}[H]
  \centering
  \includegraphics[scale=1]{figs/k_fold.pdf}
  \caption{One split of 5-fold cross validation}
\end{figure}

The $k$-fold cross-validation used in this project is \textbf{stratified}. This keeps the proportion of each class in each fold
close to the proportion of each class in the overall dataset. This is especially important when classes are unbalanced, as performing 
non-stratified cross-validation could result in biasing the model towards an uncommon class, or not including any samples of a class in the 
fold. It has also been shown by Kohavi~\cite{Kohavi:1995:SCB:1643031.1643047} stratified cross-validation generally has lower bias and variance when estimating model 
accuracies than non-stratified.

In order to perform hyperparameter optimisation (Section~\ref{hyper}) there needs to be a validation dataset, to 
compare the performance of each set of hyperparameters. Performing hyperparameter optimisation on the test set will lead to overestimating 
the ability of the model to perform on unseen data, as the hyperparameters have been chosen to perform best on the test set. Therefore,
another split has to be made to generate a validation set. The most common way to do this is \textbf{nested k-fold cross validation}:
partition the training data into another $k$ folds; compute the validation performance for each set of hyperparameters $k$ times; take the 
hyperparameters that perform best on average and train a model with those hyperparameters on all the training data.
However this method has a couple of problems. Firstly if there are $n$ hyperparameter sets to test over this method will take $nk^{2}$ 
iterations, and even if a model is quick to train the time this takes quickly becomes very large. It also means that there is no validation set used 
for the final computation of the model. As I am using early stopping a validation set is always required because the number of epochs 
to train to get best performance can be quite variable and depend on the model weight initialisation.

Therefore I instead partitioned the $kth$ fold in two, into a validation set and a test set. The hyperparameters are optimised using the 
validation set, and the performance of each model is compared. The best performing model is then run on the test set and the acccuracy
recorded. The test and validation sets are then swapped and optimisation is performed again. This reduces the number of iterations to
$2nk$ and also ensures that there is always a validation set to perform early stopping, while still using all the available data as train, 
validation, or test data at some point.
\begin{figure}[H]
  \centering
  \includegraphics[scale=1]{figs/test_val_split.pdf}
  \caption{Splitting test set into a test and val set for hyperparameter optimisation}
\end{figure}

\subsubsection{Labelled splits}
While the datasets used in this project include labels for all the samples, the focus of this project is on semi-supervised learning.
In order to give an overall view of the performance for each dataset I performed evaluation of the models with different amounts of
labelled data. For each number of labelled samples ($n$) used, I selected $n$ samples from the train set in a stratified way to use as the 
labelled dataset. The unlabelled dataset was then all the remaining samples in the train set, and these were assigned the dummy label $-1$ to 
ensure that they could not accidentally be used in supervised training.

\subsubsection{Parallelisation}
While the cross-validation changes made above reduce the number of iterations to a more manageable size, even a model with a fairly short 
training time will take a long time to complete the evalutaion. To this end I computed the indices of test/val/train and labelled 
splits I would use in advance, and serialised these using \texttt{pickle}. This allows all the models to use the exact same folds, allowing
for a fair comparison. It also allows each script to run hyperparameter optimisation over only one fold. The Cambridge High Performance
Cluster has 90 GPUs, and so running many smaller jobs in parallel results in a much quicker finish time than running several larger jobs.
To this end I also wrote several \texttt{bash} scripts to schedule slurm jobs for each model, number of labelled examples and fold. 

\section{Saliency}
Neural networks often act as a black box, with no indication to the programmer or user of what the network is doing, and what it deems 
to be important. One way of extracting that is through the use of saliency maps (first introduced in~\cite{DBLP:journals/corr/SimonyanVZ13}), 
which intend to give the user some idea of which inputs 
are the most important in determining the output class. The way this is done is by computing the partial derivatives of the input data 
with respect to the output class of the network. If the input values have been scaled to have similar ranges then this partial derivative
should be larger for for inputs that are more important to determining the class. A large positive partial derivative means that increasing 
the value of that input makes the class more likely, and a large negative partial derivative means increasing that input decreases the 
likelihood of that class.

A variant of this uses guided backpropagation~\cite{DBLP:journals/corr/SpringenbergDBR14}, a variant of backpropagation where the 
gradient is only backpropagated through a ReLU if the error gradient is greater than zero. When used to construct saliency maps for 
images it has been shown to make much clearer images, and highlight better the features the network thinks are important.

I implemented both of these methods, basing my implementation on this very simple implementation: https://github.com/Ema93sh/pytorch-saliency.
The guided backpropagation version works by registering a hook on each ReLU module that sets the gradient being backpropagated through it 
to zero if it is less than zero.

\section{Pipeline}

Since the stated aim of this project was to build a tool, the \texttt{main.py} file in the repository provides a command line tool 
for building a semi-supervised learning model. This tool takes in a file containing labelled and unlabelled data, extracts the data 
and trains a ladder network and M2 model.

\section{Repository overview}

The top level of the repository contains the file \texttt{main.py}, and the folders \texttt{scripts}, \texttt{utils}, \texttt{Saliency},
\texttt{Models} and \texttt{data}. The \texttt{scripts} folder contains python and bash scripts that can be used to run the models with the correct 
arguments to replicate the results seen in the Evaluation section. \texttt{utils} contains code for loading data, along with making the 
folds to be used in evaluation.  \texttt{Models} contains all the code for the models implemented in this project, while \texttt{data} 
is the data files and generated data splits. \texttt{Saliency} is all the code for the saliency computations, and contains the license 
from the repository it was based on.

\chapter{Evaluation}
In this section I will compare the performance of the semi-supervised models on both the MNIST and TCGA datasets, and will compare these 
to a fully supervised MLP operating on only the labelled data. I then discuss how these factored into the choices made for a more general
tool.

\section{MNIST results}
The MNIST dataset has a designated train and test set, so to obtain the results for this I performed 5-fold cross validation over the 
training set, using 48,000 examples for testing and 12,000 for validation and then computing the accuracy of the model on the 10,000
test samples. The results obtained for this were:
\begin{table}[H]
  \label{tab:mnist}
  \small % text size of table content
  \centering % center the table
  \begin{tabular}{R|CCCCC} % alignment of each column data
  \toprule[\heavyrulewidth]\toprule[\heavyrulewidth]
  & \multicolumn{5}{c}{\textbf{Models}}\\
  \shortstack{Number of \\ labelled samples} & \textbf{MLP} & \textbf{SDAE} & \textbf{M1} & \textbf{M2} & \textbf{Ladder} \\ 
  \midrule
  100 & 71.75 \pm 0.82 & 74.86 \pm 0.85 & 49.00 \pm 0.98 & 92.72 \pm 0.51 & 96.64 \pm 0.35\\
  1000 & 88.86 \pm 0.62 & 91.97 \pm 0.53 & 83.44 \pm 0.73 & 96.46 \pm 0.36 & 98.03 \pm 0.27\\
  3000 & 93.88 \pm 0.47 & 95.60 \pm 0.40 & 88.55 \pm 0.62 & 96.86 \pm 0.34 & 98.35 \pm 0.25\\
  \text{All} & 98.41 \pm 0.25 & 98.58 \pm 0.23 & 90.30 \pm 0.58 & 97.72 \pm 0.29 & 98.95 \pm 0.20\\
  \bottomrule[\heavyrulewidth] 
  \end{tabular}
  \caption{MNIST 5-fold cross-validation percentage accuracies} 
\end{table}

\subsection{Performance comparisons}

The MLP and SDAE do not have standard accuracies that can be obtained from papers, but the slight performance boost the SDAE provides 
over the MLP when only some of the data is labelled is what was expected.

The results for M1 are much worse than those in the original Kingma paper~\cite{DBLP:journals/corr/KingmaRMW14}, and this is due to the use
of a neural network as the classifier on the latent dimension. This neural network can only use the labelled samples, while the Kingma 
paper uses a transductive support vector machine, which is able to utilise the unlabelled samples in seperating the classes. In order to
not overcomplicate the project I decided against implementing a TSVM and so the results are poor. Part of the reason the results are worse 
than even the basic MLP are that the supervised learning cannot effect the weights of the VAE. If the VAE is encoding useless information 
the supervised update steps are unable to prevent this.

The results for M2 are comparable to those in the Kingma paper, and for the 100 labelled samples are actually better, averaging 92.72 
compared to 88.03. I believe that these improvements are due to performing hyperparameter optimisation over the latent dimension of the 
model, whereas the Kingma paper used a fixed latent size of 50.

The ladder results are again comparable, with my implementation averaging 96.64 compared to 98.94 by Rasmus~\cite{DBLP:journals/corr/RasmusVHBR15}.
The discrepancy is likely due to the slight differences in model structure. While I optimise over 1 to 4 hidden layers of fixed size,
while Rasmus uses 5 hidden layers with an overcomplete (1000 neurons) first hidden layer.

\section{TCGA results}

\subsection{Missing data}

As some of the samples in the TCGA dataset are missing expression levels for certain genes I decided to compare the results (using an MLP 
and all of the data as labelled data) of three different techniques for removing the missing values. The first technique was to just drop
all the columns of genes with any missing values, reducing the number of genes from 20,350 to 16334. The second technique involved 
replacing all the missing gene expression values with zero, and the third involved replacing them with the mean of the expression 
level for all the samples. The results are summarized in the table below:
\begin{table}[H]
  \label{tab:imputation}
  \small % text size of table content
  \centering % center the table
  \begin{tabular}{CCC} % alignment of each column data
  \toprule[\heavyrulewidth]\toprule[\heavyrulewidth]
  \textbf{Drop genes} & \textbf{Zero} & \textbf{Mean} \\ 
  \midrule
  95.09 \pm 0.40 & 95.67 \pm 0.38 & 95.81 \pm 0.37 \\
  \bottomrule[\heavyrulewidth] 
  \end{tabular}
  \caption{TCGA data imputation 10-fold cross-validation percentage accuracies} 
\end{table}

Computing a paired t-test helps discern if any one method could be considered better.

\begin{table}[H]
  \label{tab:ttest}
  \small % text size of table content
  \centering % center the table
  \begin{tabular}{CCC} % alignment of each column data
  \toprule[\heavyrulewidth]\toprule[\heavyrulewidth]
  \textbf{Mean/Drop genes} & \textbf{Mean/Zero} & \textbf{Drop genes/Zero} \\ 
  \midrule
  1.723 & 0.543 & 1.753 \\
  \bottomrule[\heavyrulewidth] 
  \end{tabular}
  \caption{t statistics for difference between imputation folds} 
\end{table}

None of these t statistics are statistically significant using a two-tailed t-test with p=0.05, and so we cannot reject the null hypothesis
that all the imputation methods have the same performance. However, dropping the genes makes the least sense, as that way some real data is
lost. In the command line tool produced at the end any imputation is done using the mean value. 

\subsection{Semi-supervised comparison}

\section{Saliency}

\chapter{Conclusion}

% \section{Verbatim text}

% Verbatim text can be included using \verb|\begin{verbatim}| and
% \verb|\end{verbatim}|. I normally use a slightly smaller font and
% often squeeze the lines a little closer together, as in:

% {\renewcommand{\baselinestretch}{0.8}\small
% \begin{verbatim}
% GET "libhdr"
 
% GLOBAL { count:200; all  }
 
% LET try(ld, row, rd) BE TEST row=all
%                         THEN count := count + 1
%                         ELSE { LET poss = all & ~(ld | row | rd)
%                                UNTIL poss=0 DO
%                                { LET p = poss & -poss
%                                  poss := poss - p
%                                  try(ld+p << 1, row+p, rd+p >> 1)
%                                }
%                              }
% LET start() = VALOF
% { all := 1
%   FOR i = 1 TO 12 DO
%   { count := 0
%     try(0, 0, 0)
%     writef("Number of solutions to %i2-queens is %i5*n", i, count)
%     all := 2*all + 1
%   }
%   RESULTIS 0
% }
% \end{verbatim}
% }

% \section{Tables}

% \begin{samepage}
% Here is a simple example\footnote{A footnote} of a table.

% \begin{center}
% \begin{tabular}{l|c|r}
% Left      & Centred & Right \\
% Justified &         & Justified \\[3mm]
% %\hline\\%[-2mm]
% First     & A       & XXX \\
% Second    & AA      & XX  \\
% Last      & AAA     & X   \\
% \end{tabular}
% \end{center}

% \noindent
% There is another example table in the proforma.
% \end{samepage}

% \section{Simple diagrams}

% Simple diagrams can be written directly in \LaTeX.  For example, see
% figure~\ref{latexpic1} on page~\pageref{latexpic1} and see
% figure~\ref{latexpic2} on page~\pageref{latexpic2}.

% \begin{figure}
% \setlength{\unitlength}{1mm}
% \begin{center}
% \begin{picture}(125,100)
% \put(0,80){\framebox(50,10){AAA}}
% \put(0,60){\framebox(50,10){BBB}}
% \put(0,40){\framebox(50,10){CCC}}
% \put(0,20){\framebox(50,10){DDD}}
% \put(0,00){\framebox(50,10){EEE}}

% \put(75,80){\framebox(50,10){XXX}}
% \put(75,60){\framebox(50,10){YYY}}
% \put(75,40){\framebox(50,10){ZZZ}}

% \put(25,80){\vector(0,-1){10}}
% \put(25,60){\vector(0,-1){10}}
% \put(25,50){\vector(0,1){10}}
% \put(25,40){\vector(0,-1){10}}
% \put(25,20){\vector(0,-1){10}}

% \put(100,80){\vector(0,-1){10}}
% \put(100,70){\vector(0,1){10}}
% \put(100,60){\vector(0,-1){10}}
% \put(100,50){\vector(0,1){10}}

% \put(50,65){\vector(1,0){25}}
% \put(75,65){\vector(-1,0){25}}
% \end{picture}
% \end{center}
% \caption{A picture composed of boxes and vectors.}
% \label{latexpic1}
% \end{figure}

% \begin{figure}
% \setlength{\unitlength}{1mm}
% \begin{center}

% \begin{picture}(100,70)
% \put(47,65){\circle{10}}
% \put(45,64){abc}

% \put(37,45){\circle{10}}
% \put(37,51){\line(1,1){7}}
% \put(35,44){def}

% \put(57,25){\circle{10}}
% \put(57,31){\line(-1,3){9}}
% \put(57,31){\line(-3,2){15}}
% \put(55,24){ghi}

% \put(32,0){\framebox(10,10){A}}
% \put(52,0){\framebox(10,10){B}}
% \put(37,12){\line(0,1){26}}
% \put(37,12){\line(2,1){15}}
% \put(57,12){\line(0,2){6}}
% \end{picture}

% \end{center}
% \caption{A diagram composed of circles, lines and boxes.}
% \label{latexpic2}
% \end{figure}



% \section{Adding more complicated graphics}

% The use of \LaTeX\ format can be tedious and it is often better to use
% encapsulated postscript (EPS) or PDF to represent complicated graphics.
% Figure~\ref{epsfig} and~\ref{xfig} on page \pageref{xfig} are
% examples. The second figure was drawn using \texttt{xfig} and exported in
% {\tt.eps} format. This is my recommended way of drawing all diagrams.


% \begin{figure}[tbh]
% \centerline{\includegraphics{figs/cuarms.pdf}}
% \caption{Example figure using encapsulated postscript}
% \label{epsfig}
% \end{figure}

% \begin{figure}[tbh]
% \vspace{4in}
% \caption{Example figure where a picture can be pasted in}
% \label{pastedfig}
% \end{figure}


% \begin{figure}[tbh]
% \centerline{\includegraphics{figs/diagram.pdf}}
% \caption{Example diagram drawn using \texttt{xfig}}
% \label{xfig}
% \end{figure}

% \section{Printing and binding}

% Use a ``duplex'' laser printer that can print on both sides to print
% two copies of your dissertation. Then bind them, for example using the
% comb binder in the Computer Laboratory Library.

% \section{Further information}

% See the Unix Tools notes at

% \url{http://www.cl.cam.ac.uk/teaching/current-1/UnixTools/materials.html}

% I hope that this rough guide to writing a dissertation is \LaTeX\ has
% been helpful and saved you time.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % the appendices
% \appendix

% \chapter{Latex source}

% \section{diss.tex}
% {\scriptsize\verbatiminput{diss.tex}}

% \section{proposal.tex}
% {\scriptsize\verbatiminput{proposal.tex}}

% \chapter{Makefile}

% \section{makefile}\label{makefile}
% {\scriptsize\verbatiminput{makefile.txt}}

% \section{refs.bib}
% {\scriptsize\verbatiminput{refs.bib}}

\newpage

\appendix

\chapter{Backpropagation} \label{backprop}

Backpropagation is the flow of information from the outputs of the network back through the network. Once the cost function has been computed
finding the gradient of the loss with respect to each trainable parameter shows which direction the parameter can be shifted to decrease 
the loss function.. This derivation of the backpropagation algorithm is based on that found in \textit{Artificial intelligence I}~\cite{Art_Int}.
In the derivations below:
\begin{itemize}
  \item $J(\vec{\theta})_k$ is the cost function for the $kth$ sample in the training set
  \item $w_{i \to j}$ is the weight between node $i$ and node $j$
  \item $a_j$ is the value computed by the node $j$ pre-activation ($\sum_{k} w_{k \to j} z_k$)
  \item $z_j$ and $y_j$ are the values post-activation ($\sigma(a_j)$), for non-output and output nodes respectively
\end{itemize}

The value to be calculated for each weight (per sample) is $\frac{\partial J(\vec{\theta})_k}{\partial w_{i \to j}}$. To do this we use the chain rule
of differentiation:
\begin{align}
  \frac{\partial J(\vec{\theta})_k}{\partial w_{i \to j}} & = \frac{\partial J(\vec{\theta})_k}{\partial a_j} \frac{\partial a_j}{\partial w_{i \to j}} \\
  & = \frac{\partial J(\vec{\theta})_k}{\partial a_j} z_i
\end{align}

Then for weights connected to the output nodes:
\begin{align}
  \frac{\partial J(\vec{\theta})_k}{\partial w_{i \to j}} & = \frac{\partial J(\vec{\theta})_k}{\partial a_j} z_i \\
  & = \frac{\partial J(\vec{\theta})_k}{\partial y_j} \frac{\partial y_j}{\partial a_j} z_i
\end{align}

If our loss function is differentiable with respect to $y_j$, and $y_j$ is differetiable with respect to $a_j$ then we can calculate this.  
For backpropagation to work all our loss and activation functions must have this property. Thankfully all the loss and activation functions 
shown in this dissertation do have that property and so work correctly with backpropagation.

Once the gradients are computed for the weights in the output layer it is possible to calculate them for the next layer, and then the
layer after that, etc., as the errors are backpropagated through the network. The computation for the hidden layers is slightly more 
complicated:
\begin{align}
  \frac{\partial J(\vec{\theta})_k}{\partial w_{i \to j}} & = \frac{\partial J(\vec{\theta})_k}{\partial a_j} z_i \\
  & = \left( \sum_{k \in K} \frac{\partial J(\vec{\theta})_k}{\partial a_k} \frac{\partial a_k}{\partial a_j} \right) z_i
\end{align}
\begin{center}
  \textit{where K is the set of all nodes $j$ is connected to in the next layer}
\end{center}

By definition:
\begin{align}
  \frac{\partial a_k}{\partial a_j} & = \frac{\partial}{\partial a_j} \left( \sum_{i} w_{i \to k} z_i \right) \\
  & = \frac{\partial}{\partial a_j} \left( \sum_{i} w_{i \to k} \sigma(a_i) \right) \\
  & = w_{j \to k} \sigma'(a_j)
\end{align}

Therefore, for hidden layers:
\begin{align}
  \frac{\partial J(\vec{\theta})_k}{\partial w_{i \to j}} & = \left( \sum_{k \in K} w_{j \to k} \frac{\partial J(\vec{\theta})_k}{\partial a_k} \right) \sigma'(a_j) z_i
\end{align}

As the errors are moving backward through the network, and all nodes $k$ are in the layer after $j$, $\frac{\partial J(\vec{\theta})_k}{\partial a_k}$
has already been computed and so this gradient can be calculated.

\chapter{The reparameterization trick} \label{reparam}

It is not possible to backpropagate the reconstruction loss of a varitaional autoencoder through the random sampling in the latent dimension, because the drawing of
the samples is not differentiable with respect to $\vec{\mu}$ and $\vec{\sigma}$. Previously this was solved using a Monte Carlo gradient estimator, but these have 
extremely high variance and are impractical.Instead the reparameterization trick allows the Gaussian sample to be reparameterized as:
\begin{align}
  \vec{z_{i}} = \vec{\mu} + \vec{\sigma}\vec{\epsilon}
\end{align}

$\vec{\epsilon}$ is a vector of samples from the standard normal distribution $\mathcal{N}(0, 1)$. The sampling operation is now differentiable
with respect to $\vec{\mu}$ and $\vec{\sigma}$ and therefore is differentiable with respect to $\vec{\theta}$, the trainable parameters of the encoder. As the
gradients can be backpropagated through the network it is possible to train the VAE using gradient descent.

\chapter{Project Proposal} \label{proposal}
\input{proposal}

\end{document}
